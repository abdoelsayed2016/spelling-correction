{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oMty1YwuWHpN"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurflor23/text-correction/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP-v0E_S-mQP",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/arthurflor23/text-correction/blob/master/doc/image/header.png?raw=true\">\n",
        "\n",
        "# Text Correction using TensorFlow 2.0\n",
        "\n",
        "This tutorial shows how you can use the project [Text Correction](https://github.com/arthurflor23/text-corretion) in your Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMty1YwuWHpN",
        "colab_type": "text"
      },
      "source": [
        "## 1 Localhost Environment\n",
        "\n",
        "We'll make sure you have the project in your Google Drive with the datasets folders. If you already have structured files in the cloud, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39blvPTPQJpt",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Datasets\n",
        "\n",
        "The datasets that you can use:\n",
        "\n",
        "a. [BEA2019](https://www.cl.cam.ac.uk/research/nl/bea2019st/)\n",
        "\n",
        "b. [Bentham](http://transcriptorium.eu/datasets/bentham-collection/)\n",
        "\n",
        "c. [CoNLL13](https://www.comp.nus.edu.sg/~nlp/conll13st.html)\n",
        "\n",
        "d. [CoNLL14](https://www.comp.nus.edu.sg/~nlp/conll14st.html)\n",
        "\n",
        "e. [Google](https://ai.google/research/pubs/pub41880)\n",
        "\n",
        "f. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n",
        "\n",
        "g. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n",
        "\n",
        "h. [Washington](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/washington-database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVBGMLifWQwl",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Raw folder\n",
        "\n",
        "On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n",
        "\n",
        "```\n",
        ".\n",
        "├── raw\n",
        "│   ├── bea2019\n",
        "│   │   ├── json\n",
        "│   │   ├── json_to_m2.py\n",
        "│   │   ├── licence.wi.txt\n",
        "│   │   ├── license.locness.txt\n",
        "│   │   ├── m2\n",
        "│   │   └── readme.txt\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── conll13\n",
        "│   │   ├── m2scorer\n",
        "│   │   ├── original\n",
        "│   │   ├── README\n",
        "│   │   ├── revised\n",
        "│   │   └── scripts\n",
        "│   ├── conll14\n",
        "│   │   ├── alt\n",
        "│   │   ├── noalt\n",
        "│   │   ├── README\n",
        "│   │   └── scripts\n",
        "│   ├── google\n",
        "│   │   ├── europarl-v6.cs\n",
        "│   │   ├── europarl-v6.de\n",
        "│   │   ├── europarl-v6.en\n",
        "│   │   ├── europarl-v6.es\n",
        "│   │   ├── europarl-v6.fr\n",
        "│   │   ├── news.2007.cs.shuffled\n",
        "│   │   ├── news.2007.de.shuffled\n",
        "│   │   ├── news.2007.en.shuffled\n",
        "│   │   ├── news.2007.es.shuffled\n",
        "│   │   ├── news.2007.fr.shuffled\n",
        "│   │   ├── news.2008.cs.shuffled\n",
        "│   │   ├── news.2008.de.shuffled\n",
        "│   │   ├── news.2008.en.shuffled\n",
        "│   │   ├── news.2008.es.shuffled\n",
        "│   │   ├── news.2008.fr.shuffled\n",
        "│   │   ├── news.2009.cs.shuffled\n",
        "│   │   ├── news.2009.de.shuffled\n",
        "│   │   ├── news.2009.en.shuffled\n",
        "│   │   ├── news.2009.es.shuffled\n",
        "│   │   ├── news.2009.fr.shuffled\n",
        "│   │   ├── news.2010.cs.shuffled\n",
        "│   │   ├── news.2010.de.shuffled\n",
        "│   │   ├── news.2010.en.shuffled\n",
        "│   │   ├── news.2010.es.shuffled\n",
        "│   │   ├── news.2010.fr.shuffled\n",
        "│   │   ├── news.2011.cs.shuffled\n",
        "│   │   ├── news.2011.de.shuffled\n",
        "│   │   ├── news.2011.en.shuffled\n",
        "│   │   ├── news.2011.es.shuffled\n",
        "│   │   ├── news.2011.fr.shuffled\n",
        "│   │   ├── news-commentary-v6.cs\n",
        "│   │   ├── news-commentary-v6.de\n",
        "│   │   ├── news-commentary-v6.en\n",
        "│   │   ├── news-commentary-v6.es\n",
        "│   │   └── news-commentary-v6.fr\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── __init__.py\n",
        "    │   ├── m2.py\n",
        "    │   └── preproc.py\n",
        "    ├── main.py\n",
        "    ├── tool\n",
        "    │   ├── __init__.py\n",
        "    │   ├── symspell.py \n",
        "    │   └── transformer.py\n",
        "    ├── transform\n",
        "    │   ├── bea2019.py\n",
        "    │   ├── bentham.py\n",
        "    │   ├── conll13.py\n",
        "    │   ├── conll14.py\n",
        "    │   ├── google.py\n",
        "    │   ├── iam.py\n",
        "    │   ├── __init__.py\n",
        "    │   ├── rimes.py\n",
        "    │   └── washington.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "After that, create virtual environment and install the dependencies with python 3 and pip:\n",
        "\n",
        "> ```python -m venv .venv && source .venv/bin/activate```\n",
        "\n",
        "> ```pip install -r requirements.txt```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLRbAwsWSYA",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Dataset folders\n",
        "\n",
        "Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n",
        "\n",
        "> ```python main.py --dataset=<DATASET_NAME> --transform```\n",
        "\n",
        "To work with **all** datasets, type it:\n",
        "> ```python main.py --transform```\n",
        "\n",
        "Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n",
        "\n",
        "\n",
        "```\n",
        ".\n",
        "├── data\n",
        "│   ├── all.txt\n",
        "│   ├── bea2019.txt\n",
        "│   ├── bentham.txt\n",
        "│   ├── conll13.txt\n",
        "│   ├── conll14.txt\n",
        "│   ├── google.txt\n",
        "│   ├── iam.txt\n",
        "│   ├── rimes.txt\n",
        "│   └── washington.txt\n",
        "├── raw\n",
        "│   ├── bea2019\n",
        "│   │   ├── json\n",
        "│   │   ├── json_to_m2.py\n",
        "│   │   ├── licence.wi.txt\n",
        "│   │   ├── license.locness.txt\n",
        "│   │   ├── m2\n",
        "│   │   └── readme.txt\n",
        "│   ├── bentham\n",
        "│   │   ├── BenthamDatasetR0-GT\n",
        "│   │   └── BenthamDatasetR0-Images\n",
        "│   ├── conll13\n",
        "│   │   ├── m2scorer\n",
        "│   │   ├── original\n",
        "│   │   ├── README\n",
        "│   │   ├── revised\n",
        "│   │   └── scripts\n",
        "│   ├── conll14\n",
        "│   │   ├── alt\n",
        "│   │   ├── noalt\n",
        "│   │   ├── README\n",
        "│   │   └── scripts\n",
        "│   ├── google\n",
        "│   │   ├── europarl-v6.cs\n",
        "│   │   ├── europarl-v6.de\n",
        "│   │   ├── europarl-v6.en\n",
        "│   │   ├── europarl-v6.es\n",
        "│   │   ├── europarl-v6.fr\n",
        "│   │   ├── news.2007.cs.shuffled\n",
        "│   │   ├── news.2007.de.shuffled\n",
        "│   │   ├── news.2007.en.shuffled\n",
        "│   │   ├── news.2007.es.shuffled\n",
        "│   │   ├── news.2007.fr.shuffled\n",
        "│   │   ├── news.2008.cs.shuffled\n",
        "│   │   ├── news.2008.de.shuffled\n",
        "│   │   ├── news.2008.en.shuffled\n",
        "│   │   ├── news.2008.es.shuffled\n",
        "│   │   ├── news.2008.fr.shuffled\n",
        "│   │   ├── news.2009.cs.shuffled\n",
        "│   │   ├── news.2009.de.shuffled\n",
        "│   │   ├── news.2009.en.shuffled\n",
        "│   │   ├── news.2009.es.shuffled\n",
        "│   │   ├── news.2009.fr.shuffled\n",
        "│   │   ├── news.2010.cs.shuffled\n",
        "│   │   ├── news.2010.de.shuffled\n",
        "│   │   ├── news.2010.en.shuffled\n",
        "│   │   ├── news.2010.es.shuffled\n",
        "│   │   ├── news.2010.fr.shuffled\n",
        "│   │   ├── news.2011.cs.shuffled\n",
        "│   │   ├── news.2011.de.shuffled\n",
        "│   │   ├── news.2011.en.shuffled\n",
        "│   │   ├── news.2011.es.shuffled\n",
        "│   │   ├── news.2011.fr.shuffled\n",
        "│   │   ├── news-commentary-v6.cs\n",
        "│   │   ├── news-commentary-v6.de\n",
        "│   │   ├── news-commentary-v6.en\n",
        "│   │   ├── news-commentary-v6.es\n",
        "│   │   └── news-commentary-v6.fr\n",
        "│   ├── iam\n",
        "│   │   ├── ascii\n",
        "│   │   ├── forms\n",
        "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
        "│   │   ├── lines\n",
        "│   │   └── xml\n",
        "│   ├── rimes\n",
        "│   │   ├── eval_2011\n",
        "│   │   ├── eval_2011_annotated.xml\n",
        "│   │   ├── training_2011\n",
        "│   │   └── training_2011.xml\n",
        "│   ├── saintgall\n",
        "│   │   ├── data\n",
        "│   │   ├── ground_truth\n",
        "│   │   ├── README.txt\n",
        "│   │   └── sets\n",
        "│   └── washington\n",
        "│       ├── data\n",
        "│       ├── ground_truth\n",
        "│       ├── README.txt\n",
        "│       └── sets\n",
        "└── src\n",
        "    ├── data\n",
        "    │   ├── evaluation.py\n",
        "    │   ├── generator.py\n",
        "    │   ├── __init__.py\n",
        "    │   ├── m2.py\n",
        "    │   └── preproc.py\n",
        "    ├── main.py\n",
        "    ├── tool\n",
        "    │   ├── __init__.py\n",
        "    │   ├── symspell.py \n",
        "    │   └── transformer.py\n",
        "    ├── transform\n",
        "    │   ├── bea2019.py\n",
        "    │   ├── bentham.py\n",
        "    │   ├── conll13.py\n",
        "    │   ├── conll14.py\n",
        "    │   ├── google.py\n",
        "    │   ├── iam.py\n",
        "    │   ├── __init__.py\n",
        "    │   ├── rimes.py\n",
        "    │   └── washington.py\n",
        "    └── tutorial.ipynb\n",
        "\n",
        "```\n",
        "\n",
        "Then upload the **data** and **src** folders in the same directory in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jydsAcWgWVth",
        "colab_type": "text"
      },
      "source": [
        "## 2 Google Drive Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk3e7YJiXzSl",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7twXyNGXtbJ",
        "colab_type": "text"
      },
      "source": [
        "Make sure the jupyter notebook is using GPU mode. Try to use **Tesla T4** instead of Tesla K80 (faster)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHw4tODULT1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJECz8H8XVCY",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll install TensorFlow 2.0 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMg-B5PH9h3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q gast==0.2.2 tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ukHtpZiz0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "\n",
        "print(f\"Found GPU at: {device_name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMv5wyDXxqc",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gj6qwoX9W3",
        "colab_type": "text"
      },
      "source": [
        "Mount your Google Drive partition.\n",
        "\n",
        "**Note:** *\\\"Colab Notebooks/text-correction/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQn1iBF9k9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"./gdrive\", force_remount=True)\n",
        "\n",
        "%cd \"./gdrive/My Drive/Colab Notebooks/text-correction/src/\"\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwogUA8RZAyp",
        "colab_type": "text"
      },
      "source": [
        "After mount, you can see the list os files in the project folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fj7fSngY1IX",
        "colab_type": "text"
      },
      "source": [
        "## 3 Set Python Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Q4cOlWhNl3",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvqL2Eq5ZUc7",
        "colab_type": "text"
      },
      "source": [
        "First, let's define our environment variables.\n",
        "\n",
        "Set the main configuration parameters, such as dataset, method, number of epochs and batch size. This make compatible with **main.py** and jupyter notebook:\n",
        "\n",
        "* **dataset**: \"all\", \"bea2019\", \"bentham\", \"conll13\", \"conll14\", \"google\", \"iam\", \"rimes\", \"washington\"\n",
        "\n",
        "* **mode**: type of method: \"seq2seq\", \"transformer\"; or statistical \"symspell\" (localhost only)\n",
        "\n",
        "* **epochs**: number of epochs\n",
        "\n",
        "* **batch_size**: number size of the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qpr3drnGMWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# define parameters\n",
        "dataset = \"all\"\n",
        "mode = \"seq2seq\"\n",
        "epochs = 1000\n",
        "batch_size = 64\n",
        "\n",
        "# define paths\n",
        "data_path = os.path.join(\"..\", \"data\")\n",
        "m2_src = os.path.join(data_path, f\"{dataset}.txt\")\n",
        "\n",
        "output_path = os.path.join(\"..\", \"output\", dataset, mode)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# define number max of chars per line and list of valid chars\n",
        "max_text_length = 128\n",
        "charset_base = \"\".join([chr(i) for i in range(32, 127)])\n",
        "charset_special = \"\".join([chr(i) for i in range(192, 256)])\n",
        "\n",
        "print(\"output\", output_path)\n",
        "print(\"charset:\", charset_base + charset_special)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFextshOhTKr",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 DataGenerator Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfZ1mfvsanu1",
        "colab_type": "text"
      },
      "source": [
        "The second class is **DataGenerator()**, responsible for:\n",
        "\n",
        "* Load the dataset partitions (train, valid, test);\n",
        "\n",
        "* Manager batchs for train/validation/test process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k9vpNzMIAi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data.generator import DataGenerator\n",
        "\n",
        "dtgen = DataGenerator(m2_src=m2_src,\n",
        "                      batch_size=batch_size,\n",
        "                      charset=(charset_base + charset_special),\n",
        "                      max_text_length=max_text_length)\n",
        "\n",
        "print(f\"Train sentences: {dtgen.total_train}\")\n",
        "print(f\"Validation sentences: {dtgen.total_valid}\")\n",
        "print(f\"Test sentences: {dtgen.total_test}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OdgNLK0hYAA",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHktk8AFcnKy",
        "colab_type": "text"
      },
      "source": [
        "In this step, the model will be created/loaded and default callbacks setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV0GreStISTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from data import preproc as pp, evaluation\n",
        "from tool.seq2seq import Seq2SeqAttention\n",
        "from tool.transformer import Transformer\n",
        "\n",
        "if mode == \"transformer\":\n",
        "    # disable one hot encode (from seq2seq) to use transformer model\n",
        "    dtgen.one_hot_process(False)\n",
        "    model = Transformer(num_layers=2, units=512, d_model=256, num_heads=4,\n",
        "                        dropout=0.1, tokenizer=dtgen.tokenizer)\n",
        "\n",
        "elif mode == \"seq2seq\":\n",
        "    # increase the amount noise value to make a incremental learning process\n",
        "    dtgen.increase_noise(0.001, from_up=0)\n",
        "    model = Seq2SeqAttention(units=128, dropout=0.1, tokenizer=dtgen.tokenizer)\n",
        "\n",
        "# set parameter `learning_rate` to customize or set `None` to get default schedule function\n",
        "model.compile(learning_rate=0.001)\n",
        "\n",
        "# save network summary\n",
        "model.summary(output_path, \"summary.txt\")\n",
        "\n",
        "# get default callbacks list and load checkpoint weights file (HDF5) if exists \n",
        "checkpoint = \"checkpoint_weights.hdf5\"\n",
        "callbacks = model.get_callbacks(logdir=output_path, hdf5=checkpoint, verbose=1)\n",
        "\n",
        "model.load_checkpoint(target=os.path.join(output_path, checkpoint))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASq6zqogG6Q",
        "colab_type": "text"
      },
      "source": [
        "## 4 Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8eBxuoogM-d",
        "colab_type": "text"
      },
      "source": [
        "To facilitate the visualization of the model's training, you can instantiate the Tensorboard. \n",
        "\n",
        "**Note**: All data is saved in the output folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPx4hRHuJGAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --reload_interval=2000 --logdir={output_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1fnz0Eugqru",
        "colab_type": "text"
      },
      "source": [
        "## 5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1mLOcqYgsO-",
        "colab_type": "text"
      },
      "source": [
        "The training process using *fit_generator()* to fit memory. After training, the information (epochs and minimum loss) is save."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P6MSoxCISlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to calculate total and average time per epoch\n",
        "start_time = time.time()\n",
        "h = model.fit_generator(generator=dtgen.next_train_batch(),\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch=dtgen.train_steps,\n",
        "                        validation_data=dtgen.next_valid_batch(),\n",
        "                        validation_steps=dtgen.valid_steps,\n",
        "                        callbacks=callbacks,\n",
        "                        shuffle=True,\n",
        "                        verbose=1)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "loss = h.history['loss']\n",
        "accuracy = h.history['accuracy']\n",
        "\n",
        "val_loss = h.history['val_loss']\n",
        "val_accuracy = h.history['val_accuracy']\n",
        "\n",
        "time_epoch = (total_time / len(accuracy))\n",
        "total_item = (dtgen.total_train + dtgen.total_valid)\n",
        "best_epoch_index = val_accuracy.index(max(val_accuracy))\n",
        "\n",
        "train_corpus = \"\\n\".join([\n",
        "    f\"Total train sentences:      {dtgen.total_train}\",\n",
        "    f\"Total validation sentences: {dtgen.total_valid}\",\n",
        "    f\"Batch:                      {dtgen.batch_size}\\n\",\n",
        "    f\"Total epochs:               {len(accuracy)}\",\n",
        "    f\"Total time:                 {total_time:.8f} sec\",\n",
        "    f\"Time per epoch:             {time_epoch:.8f} sec\",\n",
        "    f\"Time per item:              {(time_epoch / total_item):.8f} sec\\n\",\n",
        "    f\"Best epoch                  {best_epoch_index + 1}\",\n",
        "    f\"Training loss:              {loss[best_epoch_index]:.8f}\",\n",
        "    f\"Training accuracy:          {accuracy[best_epoch_index]:.8f}\\n\",\n",
        "    f\"Validation loss:            {val_loss[best_epoch_index]:.8f}\",\n",
        "    f\"Validation accuracy:        {val_accuracy[best_epoch_index]:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
        "    lg.write(train_corpus)\n",
        "    print(train_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13g7tDjWgtXV",
        "colab_type": "text"
      },
      "source": [
        "## 6 Predict and Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddO26OT-g_QK",
        "colab_type": "text"
      },
      "source": [
        "Since the goal is to correct text, the metrics (CER and WER) are calculated before and after of the correction.\n",
        "\n",
        "The predict process also using the *predict_generator()*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9iHL6tmaL_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "predicts = model.predict_generator(generator=dtgen.next_test_batch(),\n",
        "                                   steps=dtgen.test_steps,\n",
        "                                   use_multiprocessing=True,\n",
        "                                   verbose=1)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# calculate metrics (before and after)\n",
        "old_metric = evaluation.ocr_metrics(dtgen.dataset[\"test\"][\"dt\"], dtgen.dataset[\"test\"][\"gt\"])\n",
        "new_metric = evaluation.ocr_metrics(predicts, dtgen.dataset[\"test\"][\"gt\"])\n",
        "\n",
        "# generate report\n",
        "pred_corpus, eval_corpus = evaluation.report(dtgen, predicts, [old_metric, new_metric], total_time)\n",
        "\n",
        "with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
        "    lg.write(\"\\n\".join(pred_corpus))\n",
        "    print(\"\\n\".join(pred_corpus[:30]))\n",
        "\n",
        "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
        "    lg.write(eval_corpus)\n",
        "    print(eval_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}